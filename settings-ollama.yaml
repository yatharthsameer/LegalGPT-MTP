server:
  env_name: ${APP_ENV:ollama}

llm:
  mode: ollama
  max_new_tokens: 1024           # Increase token count for longer summaries to capture more details
  context_window: 3900
  temperature: 0.1                # Keep temperature low for factual responses

embedding:
  mode: ollama

ollama:
  llm_model: llama3.1
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  
  keep_alive: 5m
  tfs_z: 1.0                      # Keeping tail free sampling low for structured output
  top_k: 30                       # Reducing top-k to prioritize the most relevant tokens for factual, less varied responses
  top_p: 0.8                      # Lowering top-p slightly for focused generation with less diversity, ensuring summaries remain concise
  repeat_last_n: 64
  repeat_penalty: 1.3             # Increasing repeat penalty to avoid repetitive phrasing within legal summaries
  request_timeout: 600.0

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant
